
The following have been reloaded with a version change:
  1) openmpi/2.0.2 => openmpi/4.0.1

-------------------------------------------------------------------------------
There are messages associated with the following module(s):
-------------------------------------------------------------------------------

py-tensorflow/1.9.0_py27:
    Warning: this module requires a GPU, it won't work on CPU nodes.

-------------------------------------------------------------------------------


The following have been reloaded with a version change:
  1) cuda/10.1.168 => cuda/9.0.176


The following have been reloaded with a version change:
  1) openmpi/4.0.1 => openmpi/2.1.1

-------------------------------------------------------------------------------
There are messages associated with the following module(s):
-------------------------------------------------------------------------------

py-scipystack/1.0_py27:
    The SciPy Stack module is deprecated. Please use those individual
    modules instead: py-scipy, py-numpy, py-matplotlib, py-pandas, py-sympy

-------------------------------------------------------------------------------

RuntimeError: module compiled against API version 0xc but this version of numpy is 0xa
RuntimeError: module compiled against API version 0xc but this version of numpy is 0xa
2019-11-07 08:40:34.847560: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-07 08:40:35.065518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:08:00.0
totalMemory: 11.93GiB freeMemory: 11.82GiB
2019-11-07 08:40:35.171429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:85:00.0
totalMemory: 11.93GiB freeMemory: 11.82GiB
2019-11-07 08:40:35.171550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1
2019-11-07 08:40:44.175600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-07 08:40:44.176105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 
2019-11-07 08:40:44.176121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N N 
2019-11-07 08:40:44.176127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   N N 
2019-11-07 08:40:44.176488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11440 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:08:00.0, compute capability: 5.2)
2019-11-07 08:40:44.400561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11440 MB memory) -> physical GPU (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:85:00.0, compute capability: 5.2)
False
Starting epoch 0
Iteration 0, loss = 36.9156
Om: 86.86%, s8: 94.93% accuracy
RMSE: 0.2618, 0.8208
Iteration 10, loss = 10.7634
Iteration 20, loss = 4.7876
Iteration 30, loss = 2.1941
Iteration 40, loss = 2.3170
Iteration 50, loss = 2.9199
Iteration 60, loss = 2.4246
Iteration 70, loss = 2.4982
Iteration 80, loss = 2.8003
Iteration 90, loss = 2.7443
Iteration 100, loss = 2.0538
Iteration 110, loss = 2.8024
Iteration 120, loss = 2.5038
Iteration 130, loss = 2.7791
Iteration 140, loss = 3.2978
Iteration 150, loss = 2.7524
Iteration 160, loss = 2.6834
Iteration 170, loss = 2.5869
Iteration 180, loss = 2.5120
Iteration 190, loss = 2.4517
Iteration 200, loss = 3.2403
Iteration 210, loss = 2.7607
Iteration 220, loss = 3.2881
Iteration 230, loss = 2.2018
Iteration 240, loss = 3.2191
Iteration 250, loss = 2.9018
Iteration 260, loss = 2.7205
Iteration 270, loss = 2.7522
Iteration 280, loss = 3.0494
Iteration 290, loss = 2.5713
Iteration 300, loss = 2.2019
Iteration 310, loss = 3.1879
Iteration 320, loss = 2.6406
Iteration 330, loss = 2.2819
Iteration 340, loss = 2.1709
Iteration 350, loss = 2.9061
Iteration 360, loss = 2.1632
Iteration 370, loss = 2.8582
Iteration 380, loss = 2.3084
Iteration 390, loss = 2.4596
Iteration 400, loss = 3.2604
Iteration 410, loss = 2.4108
Iteration 420, loss = 2.3065
Iteration 430, loss = 2.8932
Iteration 440, loss = 3.6252
Iteration 450, loss = 3.3009
Iteration 460, loss = 2.3860
Iteration 470, loss = 2.7325
Iteration 480, loss = 2.9368
Iteration 490, loss = 2.7776
Iteration 500, loss = 2.0882
Iteration 510, loss = 2.0963
Iteration 520, loss = 3.2164
Iteration 530, loss = 2.7018
Iteration 540, loss = 2.6383
Iteration 550, loss = 2.2154
Iteration 560, loss = 2.4876
Iteration 570, loss = 3.0673
Iteration 580, loss = 2.2696
Iteration 590, loss = 2.6887
Iteration 600, loss = 2.6449
Iteration 610, loss = 2.2721
Iteration 620, loss = 2.4882
Iteration 630, loss = 2.5763
Iteration 640, loss = 2.1920
Iteration 650, loss = 2.9525
Iteration 660, loss = 2.5866
Iteration 670, loss = 1.9273
Iteration 680, loss = 2.6013
Iteration 690, loss = 2.8929
Iteration 700, loss = 2.1325
Iteration 710, loss = 2.5593
Iteration 720, loss = 2.2375
Iteration 730, loss = 2.3028
Iteration 740, loss = 2.6673
Iteration 750, loss = 2.2701
Iteration 760, loss = 2.6087
Iteration 770, loss = 3.5841
Iteration 780, loss = 2.4814
Iteration 790, loss = 3.6712
Iteration 800, loss = 2.4245
Iteration 810, loss = 2.4654
Iteration 820, loss = 1.8869
Iteration 830, loss = 2.5373
Iteration 840, loss = 2.6519
Iteration 850, loss = 2.6148
Iteration 860, loss = 2.7178
Iteration 870, loss = 2.7706
Iteration 880, loss = 2.4541
Iteration 890, loss = 2.6181
Iteration 900, loss = 2.7639
Iteration 910, loss = 2.1872
Iteration 920, loss = 3.2233
Iteration 930, loss = 2.1662
Iteration 940, loss = 2.4609
Iteration 950, loss = 2.3688
Iteration 960, loss = 2.3974
Iteration 970, loss = 3.3451
Iteration 980, loss = 2.7568
Iteration 990, loss = 2.2049
Iteration 1000, loss = 2.6879
Om: 8.74%, s8: 6.29% accuracy
RMSE: 0.0291, 0.0641
Iteration 1010, loss = 1.9104
Iteration 1020, loss = 2.8378
Iteration 1030, loss = 2.1094
Iteration 1040, loss = 2.3693
Iteration 1050, loss = 2.3981
Iteration 1060, loss = 3.1259
Iteration 1070, loss = 2.4199
Iteration 1080, loss = 2.5840
Iteration 1090, loss = 2.2638
Iteration 1100, loss = 2.3189
Iteration 1110, loss = 2.5457
Iteration 1120, loss = 3.1823
Iteration 1130, loss = 2.8123
Iteration 1140, loss = 3.2024
Iteration 1150, loss = 2.4549
Iteration 1160, loss = 2.5877
Iteration 1170, loss = 2.0768
Iteration 1180, loss = 2.7754
Iteration 1190, loss = 2.4328
Iteration 1200, loss = 2.6343
Iteration 1210, loss = 2.2982
Iteration 1220, loss = 2.7686
Iteration 1230, loss = 2.5068
Iteration 1240, loss = 2.6349
Iteration 1250, loss = 2.5460
Iteration 1260, loss = 2.1370
Iteration 1270, loss = 2.3681
Iteration 1280, loss = 3.1176
Iteration 1290, loss = 3.4355
Iteration 1300, loss = 4.1329
Iteration 1310, loss = 2.6951
Iteration 1320, loss = 2.1556
Iteration 1330, loss = 2.6815
Iteration 1340, loss = 2.3993
Iteration 1350, loss = 2.5163
Iteration 1360, loss = 2.4152
Iteration 1370, loss = 2.7274
Iteration 1380, loss = 2.6126
Iteration 1390, loss = 2.5536
Iteration 1400, loss = 2.8540
Iteration 1410, loss = 2.8199
Iteration 1420, loss = 2.4766
Iteration 1430, loss = 2.5575
Iteration 1440, loss = 2.3862
Starting epoch 1
Iteration 1450, loss = 2.2816
Iteration 1460, loss = 2.3298
Iteration 1470, loss = 2.5296
Iteration 1480, loss = 2.1560
Iteration 1490, loss = 2.5677
Iteration 1500, loss = 2.3934
Iteration 1510, loss = 2.4989
Iteration 1520, loss = 2.2464
Iteration 1530, loss = 2.4923
Iteration 1540, loss = 2.7918
Iteration 1550, loss = 2.9774
Iteration 1560, loss = 2.3771
Iteration 1570, loss = 2.4404
Iteration 1580, loss = 2.5498
Iteration 1590, loss = 2.4971
Iteration 1600, loss = 2.4491
Iteration 1610, loss = 2.8307
Iteration 1620, loss = 2.8548
Iteration 1630, loss = 2.6573
Iteration 1640, loss = 2.6650
Iteration 1650, loss = 3.0167
Iteration 1660, loss = 2.4327
Iteration 1670, loss = 2.4889
Iteration 1680, loss = 2.1657
Iteration 1690, loss = 2.2100
Iteration 1700, loss = 2.8999
Iteration 1710, loss = 2.4982
Iteration 1720, loss = 2.2650
Iteration 1730, loss = 2.5093
Iteration 1740, loss = 2.6059
Iteration 1750, loss = 2.2770
Iteration 1760, loss = 2.5014
Iteration 1770, loss = 3.6386
Iteration 1780, loss = 2.2423
Iteration 1790, loss = 2.5994
Iteration 1800, loss = 3.0816
Iteration 1810, loss = 2.6727
Iteration 1820, loss = 2.4726
Iteration 1830, loss = 3.0908
Iteration 1840, loss = 2.5566
Iteration 1850, loss = 2.3224
Iteration 1860, loss = 3.4409
Iteration 1870, loss = 2.5381
Iteration 1880, loss = 3.1291
Iteration 1890, loss = 2.5377
Iteration 1900, loss = 2.4466
Iteration 1910, loss = 2.8748
Iteration 1920, loss = 2.5735
Iteration 1930, loss = 2.4947
Iteration 1940, loss = 2.6728
Iteration 1950, loss = 2.1827
Iteration 1960, loss = 2.7492
Iteration 1970, loss = 2.8666
Iteration 1980, loss = 2.7099
Iteration 1990, loss = 2.0527
Iteration 2000, loss = 2.6871
Om: 9.80%, s8: 7.58% accuracy
RMSE: 0.0342, 0.0788
Iteration 2010, loss = 2.2599
Iteration 2020, loss = 3.3583
Iteration 2030, loss = 2.5903
Iteration 2040, loss = 2.4736
Iteration 2050, loss = 2.4352
Iteration 2060, loss = 2.7490
Iteration 2070, loss = 3.4710
Iteration 2080, loss = 2.9665
Iteration 2090, loss = 2.8145
Iteration 2100, loss = 2.3299
Iteration 2110, loss = 2.8852
Iteration 2120, loss = 2.1817
Iteration 2130, loss = 2.4795
Iteration 2140, loss = 2.5314
Iteration 2150, loss = 2.5906
Iteration 2160, loss = 2.2960
Iteration 2170, loss = 2.0986
Iteration 2180, loss = 2.1633
Iteration 2190, loss = 1.9012
Iteration 2200, loss = 2.5316
Iteration 2210, loss = 2.7982
Iteration 2220, loss = 2.8352
Iteration 2230, loss = 2.5673
Iteration 2240, loss = 2.5755
Iteration 2250, loss = 2.4648
Iteration 2260, loss = 2.7939
Iteration 2270, loss = 2.7034
Iteration 2280, loss = 2.3869
Iteration 2290, loss = 2.9553
Iteration 2300, loss = 3.0538
Iteration 2310, loss = 2.2128
Iteration 2320, loss = 2.3677
Iteration 2330, loss = 2.6994
Iteration 2340, loss = 3.0592
Iteration 2350, loss = 2.1429
Iteration 2360, loss = 2.1263
Iteration 2370, loss = 2.4654
Iteration 2380, loss = 2.8203
Iteration 2390, loss = 2.3656
Iteration 2400, loss = 2.6878
Iteration 2410, loss = 2.6080
Iteration 2420, loss = 3.0924
Iteration 2430, loss = 2.6948
Iteration 2440, loss = 3.5829
Iteration 2450, loss = 2.7579
Iteration 2460, loss = 2.4280
Iteration 2470, loss = 2.3958
Iteration 2480, loss = 2.3865
Iteration 2490, loss = 2.4616
Iteration 2500, loss = 1.8359
Iteration 2510, loss = 2.9703
Iteration 2520, loss = 2.6700
Iteration 2530, loss = 2.5875
Iteration 2540, loss = 2.7717
Iteration 2550, loss = 2.6261
Iteration 2560, loss = 2.4396
Iteration 2570, loss = 2.0543
Iteration 2580, loss = 2.4511
Iteration 2590, loss = 2.3161
Iteration 2600, loss = 2.7906
Iteration 2610, loss = 2.8083
Iteration 2620, loss = 2.2294
Iteration 2630, loss = 2.4854
Iteration 2640, loss = 2.5853
Iteration 2650, loss = 2.3426
Iteration 2660, loss = 2.4224
Iteration 2670, loss = 2.2144
Iteration 2680, loss = 2.6258
Iteration 2690, loss = 2.1531
Iteration 2700, loss = 2.1730
Iteration 2710, loss = 2.8846
Iteration 2720, loss = 2.4099
Iteration 2730, loss = 2.2002
Iteration 2740, loss = 2.9979
Iteration 2750, loss = 2.4525
Iteration 2760, loss = 2.6042
Iteration 2770, loss = 2.6939
Iteration 2780, loss = 2.6787
Iteration 2790, loss = 2.7096
Iteration 2800, loss = 2.6822
Iteration 2810, loss = 2.4107
Iteration 2820, loss = 2.7445
Iteration 2830, loss = 2.5022
Iteration 2840, loss = 2.1536
Iteration 2850, loss = 2.5160
Iteration 2860, loss = 2.1853
Iteration 2870, loss = 2.4281
Iteration 2880, loss = 2.8822
Starting epoch 2
Iteration 2890, loss = 2.0502
Iteration 2900, loss = 2.3822
Iteration 2910, loss = 2.4917
Iteration 2920, loss = 2.4066
Iteration 2930, loss = 2.1949
Iteration 2940, loss = 2.3801
Iteration 2950, loss = 2.2529
Iteration 2960, loss = 2.4863
Iteration 2970, loss = 2.7569
Iteration 2980, loss = 2.7678
Iteration 2990, loss = 2.2738
Iteration 3000, loss = 2.6384
Om: 9.36%, s8: 6.38% accuracy
RMSE: 0.0321, 0.0626
Iteration 3010, loss = 2.8355
Iteration 3020, loss = 2.6982
Iteration 3030, loss = 2.2872
Iteration 3040, loss = 2.1035
Iteration 3050, loss = 2.5948
Iteration 3060, loss = 2.6357
Iteration 3070, loss = 2.9494
Iteration 3080, loss = 2.4703
Iteration 3090, loss = 2.9911
Iteration 3100, loss = 2.8191
Iteration 3110, loss = 3.3674
Iteration 3120, loss = 2.3287
Iteration 3130, loss = 2.6987
Iteration 3140, loss = 2.3264
Iteration 3150, loss = 2.2905
Iteration 3160, loss = 2.2971
Iteration 3170, loss = 2.5429
Iteration 3180, loss = 2.7183
Iteration 3190, loss = 2.2364
Iteration 3200, loss = 2.4862
Iteration 3210, loss = 2.6673
Iteration 3220, loss = 2.3670
Iteration 3230, loss = 2.1235
Iteration 3240, loss = 2.8694
Iteration 3250, loss = 2.3001
Iteration 3260, loss = 3.4323
Iteration 3270, loss = 2.2966
Iteration 3280, loss = 2.0002
Iteration 3290, loss = 2.6546
Iteration 3300, loss = 2.4397
Iteration 3310, loss = 2.1159
Iteration 3320, loss = 2.8832
Iteration 3330, loss = 2.1136
Iteration 3340, loss = 2.7075
Iteration 3350, loss = 2.3669
Iteration 3360, loss = 2.7189
Iteration 3370, loss = 3.0746
Iteration 3380, loss = 2.3784
Iteration 3390, loss = 2.2157
Iteration 3400, loss = 2.0673
Iteration 3410, loss = 2.9718
Iteration 3420, loss = 2.5921
Iteration 3430, loss = 2.6766
Iteration 3440, loss = 2.2210
Iteration 3450, loss = 2.6766
Iteration 3460, loss = 2.4336
Iteration 3470, loss = 2.5685
Iteration 3480, loss = 2.2941
Iteration 3490, loss = 2.6344
Iteration 3500, loss = 2.4661
Iteration 3510, loss = 2.5357
Iteration 3520, loss = 2.3628
Iteration 3530, loss = 2.0901
Iteration 3540, loss = 2.8744
Iteration 3550, loss = 2.6624
Iteration 3560, loss = 2.0528
Iteration 3570, loss = 2.5116
Iteration 3580, loss = 3.1166
Iteration 3590, loss = 2.2587
Iteration 3600, loss = 2.4599
Iteration 3610, loss = 1.9719
Iteration 3620, loss = 2.3515
Iteration 3630, loss = 2.1692
Iteration 3640, loss = 2.3062
Iteration 3650, loss = 2.4374
Iteration 3660, loss = 2.7286
Iteration 3670, loss = 2.8761
Iteration 3680, loss = 2.2111
Iteration 3690, loss = 2.6744
Iteration 3700, loss = 2.8726
Iteration 3710, loss = 1.8980
Iteration 3720, loss = 2.7175
Iteration 3730, loss = 3.0407
Iteration 3740, loss = 2.8487
Iteration 3750, loss = 2.6956
Iteration 3760, loss = 2.7683
Iteration 3770, loss = 2.4394
Iteration 3780, loss = 2.7380
Iteration 3790, loss = 2.4682
Iteration 3800, loss = 2.1060
Iteration 3810, loss = 3.2956
Iteration 3820, loss = 2.3268
Iteration 3830, loss = 2.5290
Iteration 3840, loss = 2.3506
